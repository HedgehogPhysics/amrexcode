#include <AMReX_VisMF.H>
#include <AMReX_TagBox.H>
#include <AMReX_ParmParse.H>
#include <AMReX_GpuMemory.H>

#include <fstream>
#include <sstream>
#include <vector>
#include <string>

#include "AmrLevelAdv.H"
#include "tagging_K.H"
//#include "Prob.H"
//#include "Kernels.H"


using namespace amrex;

int      AmrLevelAdv::verbose         = 0;
Real     AmrLevelAdv::cfl             = 0.5;  // Default value - can be overwritten in settings file
int      AmrLevelAdv::do_reflux       = 1;

//int      AmrLevelAdv::NUM_STATE       = 8;  // One variable in the state
int      AmrLevelAdv::NUM_GROW        = 2;  // number of ghost cells

const Real gam = 2;
//int Dim DO LATER 



// Mechanism for getting code to work on GPU
ProbParm* AmrLevelAdv::h_prob_parm = nullptr;
ProbParm* AmrLevelAdv::d_prob_parm = nullptr;

// Parameters for mesh refinement
int      AmrLevelAdv::max_phierr_lev  = -1;
int      AmrLevelAdv::max_phigrad_lev = -1;
Vector<Real> AmrLevelAdv::phierr;
Vector<Real> AmrLevelAdv::phigrad;

/**
 * Default constructor.  Builds invalid object.
 */
AmrLevelAdv::AmrLevelAdv () = default;

/**
 * The basic constructor.
 */
AmrLevelAdv::AmrLevelAdv (Amr&            papa,
                          int             lev,
                          const Geometry& level_geom,
                          const BoxArray& bl,
                          const DistributionMapping& dm,
                          Real            time)
    :
    AmrLevel(papa,lev,level_geom,bl,dm,time)
{
    if (level > 0 && do_reflux) {
      // Flux registers store fluxes at patch boundaries to ensure fluxes are conservative between AMR levels
      // Flux registers are only required if AMR is actually used, and if flux fix up is being done (recommended)
        flux_reg = std::make_unique<FluxRegister>(grids,dmap,crse_ratio,level,NUM_STATE);
    }
}

/**
 * The destructor.
 */
AmrLevelAdv::~AmrLevelAdv () = default;

/**
 * Restart from a checkpoint file.
 */
// AMReX can save simultion state such that if the code crashes, it
// can be restarted, from the last checkpoint output (i.e. hhalfway
// through the simulation) with different settings files parameters if
// necessary (e.g. you might change the final time to output at the
// point of the crash, and investigate what is happening).
void
AmrLevelAdv::restart (Amr&          papa,
                      std::istream& is,
                      bool          bReadSpecial)
{
    AmrLevel::restart(papa,is,bReadSpecial);

    if (level > 0 && do_reflux) {
        flux_reg = std::make_unique<FluxRegister>(grids,dmap,crse_ratio,level,NUM_STATE);
    }
}

/**
 * Write a checkpoint file.
 */
void
AmrLevelAdv::checkPoint (const std::string& dir,
                         std::ostream&      os,
                         VisMF::How         how,
                         bool               dump_old)
{
    AmrLevel::checkPoint(dir, os, how, dump_old);
}

/**
 * Write a plotfile to specified directory.
 */
// Default output format is handled automatically by AMReX, and is well parallelised
void
AmrLevelAdv::writePlotFile (const std::string& dir,
                             std::ostream&      os,
                            VisMF::How         how)
{

    AmrLevel::writePlotFile (dir,os,how);

}

/**
 * Define data descriptors.
 */
// This is how the variables in a simulation are defined.  In the case
// of the advection equation, a single variable, phi, is defined.
void
AmrLevelAdv::variableSetUp ()
{
  BL_ASSERT(desc_lst.size() == 0);
  
  // Initialize struct containing problem-specific variables
  h_prob_parm = new ProbParm{};
  d_prob_parm = (ProbParm*)The_Arena()->alloc(sizeof(ProbParm));
  
  // A function which contains all processing of the settings file,
  // setting up initial data, choice of numerical methods and
  // boundary conditions
  read_params();

  const int storedGhostZones = 0;

  // Setting up a container for a variable, or vector of variables:
  // Phi_Type: Enumerator for this variable type
  // IndexType::TheCellType(): AMReX can support cell-centred and vertex-centred variables (cell centred here)
  // StateDescriptor::Point: Data can be a point in time, or an interval over time (point here)
  // storedGhostZones: Ghost zones can be stored (e.g. for output).  Generally set to zero.
  // NUM_STATE: Number of variables in the variable vector (1 in the case of advection equation)
  // cell_cons_interp: Controls interpolation between levels - cons_interp is good for finite volume
  desc_lst.addDescriptor(Phi_Type,IndexType::TheCellType(),
			 StateDescriptor::Point,storedGhostZones,NUM_STATE,
			 &cell_cons_interp);

  
  // //Set up boundary conditions, all boundaries can be set
  // //independently, including for individual variables, but lo (left) and hi (right) are useful ways to
  // //store them, for consistent access notation for the boundary
  // //locations
  // int lo_bc[BL_SPACEDIM];
  // int hi_bc[BL_SPACEDIM];

  // int lo_bc_x[BL_SPACEDIM];
  // int hi_bc_x[BL_SPACEDIM];

  // int lo_bc_y[BL_SPACEDIM];
  // int hi_bc_y[BL_SPACEDIM];

  // // AMReX has pre-set BCs, including periodic (int_dir) and transmissive (foextrap)
  // for (int i = 0; i < BL_SPACEDIM; ++i) {
  //   lo_bc[i] = hi_bc[i] = BCType::reflect_even;   // periodic boundaries
  

  // // Object for storing all the boundary conditions

  //   if (i % 2 == 0)
  //   {
  //   lo_bc_x[i] = hi_bc_x[i] = BCType::reflect_odd;
  //   }
  //   else
  //   {
  //     lo_bc_x[i] = hi_bc_x[i] = BCType::reflect_even;
  //   }

  // // Object for storing all the boundary conditions
  //   if (i % 2 == 0)
  //   {
  //   lo_bc_y[i] = hi_bc_y[i] = BCType::reflect_even;
  //   }
  //   else
  //   {
  //     lo_bc_y[i] = hi_bc_y[i] = BCType::reflect_odd;
  //   }

  // // Object for storing all the boundary conditions
  // }
  // BCRec bc(lo_bc, hi_bc);
  // BCRec bc_x(lo_bc_x, hi_bc_x);
  // BCRec bc_y(lo_bc_y, hi_bc_y);


  // // BndryFunc: Function for setting boundary conditions.  For basic
  // // BCs, AMReX can handle these automatically; nullfill means that
  // // nothing unusual is happening, which is fine for transmissive,
  // // periodic and reflective conditions, but if Dirichlet or other
  // // complex boundaries are required, this will be replaced with a
  // // boundary condition function you have written
  // StateDescriptor::BndryFunc bndryfunc(nullfill);
  // // Make sure that the GPU is happy
  // bndryfunc.setRunOnGPU(true);  // I promise the bc function will launch gpu kernels.
  
  // // Set up variable-specific information; needs to be done for each variable in NUM_STATE
  // // Phi_Type: Enumerator for the variable type being set
  // // 0: Position of the variable in the variable vector.  Single variable for advection.
  // // phi: Name of the variable - appears in output to identify what is being plotted
  // // bc: Boundary condition object for this variable (defined above)
  // // bndryfunc: The boundary condition function set above
  // desc_lst.setComponent(Phi_Type, 0, "rho", bc,
	// 		bndryfunc);

  // desc_lst.setComponent(Phi_Type, 1, "momx", bc_x,
  //     bndryfunc);

  // desc_lst.setComponent(Phi_Type, 2, "momy",   bc_y, bndryfunc);
  // desc_lst.setComponent(Phi_Type, 3, "momz",   bc, bndryfunc);
  // desc_lst.setComponent(Phi_Type, 4, "Bx",     bc_x, bndryfunc);
  // desc_lst.setComponent(Phi_Type, 5, "By",     bc_y, bndryfunc);
  // desc_lst.setComponent(Phi_Type, 6, "Bz",     bc, bndryfunc);
  // desc_lst.setComponent(Phi_Type, 7, "energy", bc, bndryfunc);
  // desc_lst.setComponent(Phi_Type, 8, "psi", bc, bndryfunc);

  int lo_bc[BL_SPACEDIM];
  int hi_bc[BL_SPACEDIM];

  for (int i = 0; i < BL_SPACEDIM; ++i) {
  lo_bc[i] = hi_bc[i] = BCType::foextrap; // Transmissive boundaries
  }

  // lo_bc[1] = hi_bc[1] = BCType::foextrap;
  // lo_bc[0] = hi_bc[0] = BCType::int_dir;
  // lo_bc[2] = hi_bc[2] = 

BCRec bc(lo_bc, hi_bc); // Reuse for all components
StateDescriptor::BndryFunc bndryfunc(nullfill);
bndryfunc.setRunOnGPU(true);

desc_lst.setComponent(Phi_Type, 0, "rho",    bc, bndryfunc);
desc_lst.setComponent(Phi_Type, 1, "momx",   bc, bndryfunc);
desc_lst.setComponent(Phi_Type, 2, "momy",   bc, bndryfunc);
desc_lst.setComponent(Phi_Type, 3, "momz",   bc, bndryfunc);
desc_lst.setComponent(Phi_Type, 4, "Bx",     bc, bndryfunc);
desc_lst.setComponent(Phi_Type, 5, "By",     bc, bndryfunc);
desc_lst.setComponent(Phi_Type, 6, "Bz",     bc, bndryfunc);
desc_lst.setComponent(Phi_Type, 7, "energy", bc, bndryfunc);
desc_lst.setComponent(Phi_Type, 8, "psi",    bc, bndryfunc);
}

/**
 * Cleanup data descriptors at end of run.
 */
void
AmrLevelAdv::variableCleanUp ()
{
    desc_lst.clear();

    // Delete structs containing problem-specific parameters
    delete h_prob_parm;
    // This relates to freeing parameters on the GPU too
    The_Arena()->free(d_prob_parm);
}


struct DataPoint {
  double x, y, z;
  double rho, ux, uy, uz, bx, by, bz, pgas;
};

/**
 * Initialize grid data at problem start-up.
 */
void
AmrLevelAdv::initData ()
{
  // amrex::Print works like std::cout, but in parallel only prints
  // from the root processor
  if (verbose) {
    amrex::Print() << "Initializing the data at level " << level << std::endl;
  }

  // GpuArrays are fixed size vectors which can be used on GPU
  // The geom object contains a lot of information about the grid, including dx...
  const GpuArray<Real, AMREX_SPACEDIM> dx = geom.CellSizeArray();
  // ... and ProbLoArray - the physical coordinates of the start of the domain
  const GpuArray<Real, AMREX_SPACEDIM> prob_lo = geom.ProbLoArray();

  // Create a local multifab which can store the initial data, and set
  // it as the global 'new_data' (i.e. data at the current time level)
  MultiFab& S_new = get_new_data(Phi_Type);
  
//   std::vector<DataPoint> data;

//   std::ifstream infile("visit_ex_db.okc");
//   if (!infile.is_open()){
//     amrex::Abort("Error: cannot open file");
//   }

//   std::string line; // Define string line

//   const int header_lines = 23;
//     for (int i = 0; i < header_lines; ++i) {
//         std::getline(infile, line);
//     }

//     while (std::getline(infile, line)) {
//       std::istringstream iss(line);
//       DataPoint dp;
//       iss >> dp.x >> dp.y >> dp.z
//           >> dp.rho >> dp.ux >> dp.uy >> dp.uz
//           >> dp.bx >> dp.by >> dp.bz >> dp.pgas;
//       data.push_back(dp);
//   } // Read each variable into data from each line. 

//   amrex::Print() << "Loaded " << data.size() << " data points from file.\n";

//   // === Get domain size from AMReX geometry ===
//   const IntVect dom_size = geom.Domain().size();
//   int nx = dom_size[0];
//   int ny = dom_size[1];

//   amrex::Print() << "AMReX domain size: nx=" << nx << " ny=" << ny << "\n";

//   // Sanity check: match data size with AMReX domain size
//   if (data.size() != nx * ny) {
//     amrex::Abort("ERROR: Data points do not match amr.n_cell grid size! "
//                  "Loaded data size: " + std::to_string(data.size()) +
//                  ", expected: " + std::to_string(nx*ny));
// }

// === Loop over the MultiFab and initialize it ===
for (MFIter mfi(S_new); mfi.isValid(); ++mfi) 
{
  const Box& box = mfi.validbox();
  const Array4<Real>& phi = S_new.array(mfi);

  // ParallelFor(box, [=,&data,nx] AMREX_GPU_DEVICE (int i, int j, int) noexcept
  // {

  //     int idx = i + nx * j;
  //     const auto& dp = data[idx];

  //     Real rho = dp.rho;
  //     Real vx  = dp.ux;
  //     Real vy  = dp.uy;
  //     Real vz  = dp.uz;
  //     Real bx  = dp.bx;
  //     Real by  = dp.by;
  //     Real bz  = dp.bz;
  //     Real pres = dp.pgas;
  //     Real psi = 0.0;

  //     // --Localisation Check --
  //     Real vacuum_threshold = 1.0e-9;
  //     if (dp.rho > vacuum_threshold)
  //     {   
  //         Real kick_velocity = 50;
  //         Real momy_kick = dp.rho * kick_velocity;

  //         Real momx = rho * vx;
  //         Real momy = rho * vy + momy_kick; // Apply the kick
  //         Real momz = rho * vz;

  //         Real E_kin = 0.5 * rho * (vx * vx + (vy + kick_velocity) * (vy + kick_velocity) + vz * vz); // Also update kinetic energy     
  //         Real E_mag = 0.5 * (bx*bx + by*by + bz*bz);
  //         Real eng = E_kin + pres / (gam - 1.0) + E_mag;

  //         phi(i,j,0) = rho;
  //         phi(i,j,1) = momx;
  //         phi(i,j,2) = momy;
  //         phi(i,j,3) = momz;
  //         phi(i,j,4) = bx;
  //         phi(i,j,5) = by;
  //         phi(i,j,6) = bz;
  //         phi(i,j,7) = eng;
  //         phi(i,j,8) = psi;
  //     }
  //     else
  //     {
  //       // --- This is a VACUUM CELL ---
  //         // Set fluid quantities to zero, but keep the magnetic field.
  //         Real rho  = 0.0;
  //         Real momx = 0.0;
  //         Real momy = 0.0;
  //         Real momz = 0.0;
  //         Real pres = 0.0; // Or a small floor value if needed

  //         Real bx   = dp.bx;
  //         Real by   = dp.by;
  //         Real bz   = dp.bz;

  //         // Energy in the vacuum is purely magnetic
  //         Real E_mag = 0.5 * (bx*bx + by*by + bz*bz);
  //         Real eng = E_mag; // No thermal or kinetic energy

  //         // Set all state variables for the vacuum cell
  //         phi(i,j,0) = rho;
  //         phi(i,j,1) = momx;
  //         phi(i,j,2) = momy;
  //         phi(i,j,3) = momz;
  //         phi(i,j,4) = bx;
  //         phi(i,j,5) = by;
  //         phi(i,j,6) = bz;
  //         phi(i,j,7) = eng;
  //         phi(i,j,8) = 0.0; // Assuming psi is zero
  //     }
  // });
  ParallelFor(box, [=] AMREX_GPU_DEVICE (int i, int j, int k) noexcept
{
    Real x = prob_lo[0] + (i + 0.5) * dx[0];
    Real y = prob_lo[1] + (j + 0.5) * dx[1];
    Real z = prob_lo[2] + (k + 0.5) * dx[2];
    
    Real rho, vx, vy, vz, bx, by, bz, pres;

    if (y <= 400)
    {
        rho  = 1.00000;
        vx   = 0.00000;
        vy   = 0.00000;
        vz   = 0.00000;
        bx   = 1.0;
        // bx = 1.00000;
        // by = 0.75000;
        by   = 0.75;
        bz   = 0.00000;
        pres = 1.00000;
    }
    else
    {
        rho  = 0.12500;
        vx   = 0.00000;
        vy   = 0.00000;
        vz   = 0.00000;
        bx   = -1.0;
        // bx = -1.00000;
        // by = 0.75000;
        by   = 0.75;
        bz   = 0.00000;
        pres = 0.10000;
    }

    Real momx = rho * vx;
    Real momy = rho * vy;
    Real momz = rho * vz;

    Real E_kin = 0.5 * rho * (vx*vx + vy*vy + vz*vz);
    Real E_mag = 0.5 * (bx*bx + by*by + bz*bz);
    Real eng   = E_kin + pres / (gam - 1.0) + E_mag;

    phi(i,j,k,0) = rho;
    phi(i,j,k,1) = momx;
    phi(i,j,k,2) = momy;
    phi(i,j,k,3) = momz;
    phi(i,j,k,4) = bx;
    phi(i,j,k,5) = by;
    phi(i,j,k,6) = bz;
    phi(i,j,k,7) = eng;
    phi(i,j,k,8) = 0.0; // psi
});
}

  if (verbose) {
    amrex::Print() << "Done initializing the level " << level
		   << " data " << std::endl;
  }
}


/**
 * Initialize data on this level from another AmrLevelAdv (during regrid).
 */
// These are standard AMReX commands which are unlikely to need altering

void
AmrLevelAdv::init (AmrLevel &old)
{
  auto* oldlev = (AmrLevelAdv*) &old;

  //
  // Create new grid data by fillpatching from old.
  //
  Real dt_new    = parent->dtLevel(level);
  Real cur_time  = oldlev->state[Phi_Type].curTime();
  Real prev_time = oldlev->state[Phi_Type].prevTime();
  Real dt_old    = cur_time - prev_time;
  setTimeLevel(cur_time,dt_old,dt_new);

  MultiFab& S_new = get_new_data(Phi_Type);

  const int zeroGhosts = 0;
  // FillPatch takes the data from the first argument (which contains
  // all patches at a refinement level) and fills (copies) the
  // appropriate data onto the patch specified by the second argument:
  // old: Source data
  // S_new: destination data
  // zeroGhosts: If this is non-zero, ghost zones could be filled too - not needed for init routines
  // cur_time: AMReX can attempt interpolation if a different time is specified - not recommended for advection eq.
  // Phi_Type: Specify the type of data being set
  // 0: This is the first data index that is to be copied
  // NUM_STATE: This is the number of states to be copied
    FillPatch(old, S_new, zeroGhosts, cur_time, Phi_Type, 0, NUM_STATE);

  // Note: In this example above, the all states in Phi_Type (which is
  // only 1 to start with) are being copied.  However, the FillPatch
  // command could be used to create a velocity vector from a
  // primitive variable vector.  In this case, the `0' argument is
  // replaced with the position of the first velocity component in the
  // primitive variable vector, and the NUM_STATE arguement with the
  // dimensionality - this argument is the number of variables that
  // are being filled/copied, and NOT the position of the final
  // component in e.g. the primitive variable vector.
}


/**
 * Initialize data on this level after regridding if old level did not previously exist
 */
// These are standard AMReX commands which are unlikely to need altering

void
AmrLevelAdv::init ()
{
  Real dt        = parent->dtLevel(level);
  Real cur_time  = getLevel(level-1).state[Phi_Type].curTime();
  Real prev_time = getLevel(level-1).state[Phi_Type].prevTime();

  Real dt_old = (cur_time - prev_time)/(Real)parent->MaxRefRatio(level-1);

  setTimeLevel(cur_time,dt_old,dt);
  MultiFab& S_new = get_new_data(Phi_Type);

  const int zeroGhosts = 0;
  // See first init function for documentation
  // Only difference is that because the patch didn't previously exist, there is no
  // 'old' entry (instead the function will interpolate from the coarse level)
  FillCoarsePatch(S_new, zeroGhosts, cur_time, Phi_Type, 0, NUM_STATE);
}

/**
 * Advance grids at this level in time.
 */
//  This function is the one that actually calls the flux functions.
Real
AmrLevelAdv::advance (Real time,
                      Real dt,
                      int  iteration,
                      int  /*ncycle*/)
{
  MultiFab& S_mm = get_new_data(Phi_Type);
  // We will only print this for level 0 to keep the output clean.
  

  // Note that some useful commands exist - the maximum and minumum
  // values on the current level can be computed directly - here the
  // max and min of variable 0 are being calculated, and output.
  Real maxval = S_mm.max(0);
  Real minval = S_mm.min(0);
  amrex::Print() << "phi max = " << maxval << ", min = " << minval  << std::endl;

  // This ensures that all data computed last time step is moved from
  // new' data to old data' - this should not need changing If more
  // than one type of data were declared in variableSetUp(), then the
  // loop ensures that all of it is updated appropriately
  for (int k = 0; k < NUM_STATE_TYPE; k++) {
    state[k].allocOldData();
    state[k].swapTimeLevels(dt);
  }

  MultiFab& S_new = get_new_data(Phi_Type);

  // Not sure about this line...
  Real ch = computeMaxWaveSpeed(S_new, gam);
  Real cp = sqrt(0.18 * ch); // Parabolic damping term

  const Real prev_time = state[Phi_Type].prevTime();
  const Real cur_time = state[Phi_Type].curTime();
  const Real ctr_time = 0.5*(prev_time + cur_time);

  GpuArray<Real,BL_SPACEDIM> dxVect = geom.CellSizeArray();
  GpuArray<Real,BL_SPACEDIM> prob_lo = geom.ProbLoArray();

  //
  // Get pointers to Flux registers, or set pointer to zero if not there.
  //
  FluxRegister *fine    = nullptr;
  FluxRegister *current = nullptr;

  int finest_level = parent->finestLevel();

  // If we are not on the finest level, fluxes may need correcting
  // from those from finer levels.  To start this process, we set the
  // flux register values to zero
  if (do_reflux && level < finest_level) {
    fine = &getFluxReg(level+1);
    fine->setVal(0.0);
  }

  // If we are not on the coarsest level, the fluxes are going to be
  // used to correct those on coarser levels.  We get the appropriate
  // flux level to include our fluxes within
  if (do_reflux && level > 0) {
    current = &getFluxReg(level);
  }

  // Set up a dimensional multifab that will contain the fluxes
  MultiFab fluxes[BL_SPACEDIM];

  // Define the appropriate size for the flux MultiFab.
  // Fluxes are defined at cell faces - this is taken care of by the
  // surroundingNodes(j) command, ensuring the size of the flux
  // storage is increased by 1 cell in the direction of the flux.
  // This is only needed if refluxing is happening, otherwise fluxes
  // don't need to be stored, just used
  for (int j = 0; j < BL_SPACEDIM; j++)
  {
    BoxArray ba = S_new.boxArray();
    ba.surroundingNodes(j);
    fluxes[j].define(ba, dmap, NUM_STATE, 0);
  }

  // Advection velocity
  const GpuArray<Real, AMREX_SPACEDIM> vel{AMREX_D_DECL(1.0, 1.0, 1.0)};

  // State with ghost cells - this is used to compute fluxes and perform the update.
  MultiFab Sborder(grids, dmap, NUM_STATE, NUM_GROW);
  // We use FillPatcher to do fillpatch here if we can
  // This is similar to the FillPatch function documented in init(), but takes care
  // of boundary conditions too
  // We will print the BCs for level 0 right before the fill operation
//   if (level == 0) {
//     amrex::Print() << "\n[DEBUG] About to call FillPatcherFill at time " << time << "\n";
//     amrex::Print() << "[DEBUG] Checking boundary conditions for state: " << Phi_Type << "\n";

//     // This gets the boundary condition records AMReX has stored for this state
//     const amrex::Vector<amrex::BCRec>& bcs_for_state = state[Phi_Type].descriptor()->getBCs();
//     // Loop through each component (rho, momx, etc.) and print its BCs
//     for (int n = 0; n < NUM_STATE; ++n) {
//         const amrex::BCRec& bc = bcs_for_state[n];
//         amrex::Print() << "  [DEBUG] Component " << n << ":\n";
//         for (int idim = 0; idim < AMREX_SPACEDIM; ++idim) {
//             amrex::Print() << "    [DEBUG] Dim " << idim << ": LO_BC = " << bc.lo(idim)
//                            << ", HI_BC = " << bc.hi(idim) << "\n";
//         }
//     }
//     amrex::Print() << "[DEBUG] End of BC check.\n\n";
// }
  FillPatcherFill(Sborder, 0, NUM_STATE, NUM_GROW, time, Phi_Type, 0);
  // Add this block for debugging
// Add this corrected block for debugging
{
  const int rho_comp = 0;
  
  for (MFIter mfi(Sborder); mfi.isValid(); ++mfi)
  {
      const Box& bx = mfi.growntilebox();
      const auto& fab = Sborder.array(mfi);

      // The fix is here: Changed [=] to [&] to capture by reference
      amrex::Loop(bx, [&] (int i, int j, int k)
      {
          // Only print for the cell at the origin to avoid huge output
          if (i == 0 && j == 0 && k == 0) {
              Real rho_val = fab(i, j, k, rho_comp);
              amrex::Print() << "CHECKING CELL BEFORE UPDATE (0,0,0) on Grid " << mfi.index() 
                             << ": rho = " << rho_val << "\n";
          }
      });
  }
}

  //const auto dxVect = geom.CellSizeArray();

  for (int d = 0; d < amrex::SpaceDim ; d++)   
  {

    const int iOffset = ( d == 0 ? 1 : 0);
    const int jOffset = ( d == 1 ? 1 : 0);
    const int kOffset = ( d == 2 ? 1 : 0);


    auto minmod = [] AMREX_GPU_HOST_DEVICE (Real r) -> Real {
      if (r <= 0.0) return 0.0;
      else if (r <= 1.0) return r;
      else return amrex::min(1.0, 2.0 / (1.0 + r));
  };
  
  Real dx = dxVect[d]; // dx for x-dir, dy for y-dir, dz for z-dir.
  
  Real dt_by_dx = dt / dx;


    MultiFab halfuR(grids, dmap, NUM_STATE, NUM_GROW);
    MultiFab halfuL(grids, dmap, NUM_STATE, NUM_GROW);


    //for (MFIter mfi(fluxes[d], true); mfi.isValid(); ++mfi)
    // This loop is going over the faces (fluxes[d]), perhaps it shouldn't. 
    for (MFIter mfi(Sborder, true); mfi.isValid(); ++mfi)
    {
      const Box& bx = grow(mfi.tilebox(), 1);
      
      //std:: cout << bx << std::endl;

      // Indexable arrays for the data, and the directional flux
      // Based on the vertex-centred definition of the flux array, the
      // data array runs from e.g. [0,N] and the flux array from [0,N+1]
      const auto& arr = Sborder.array(mfi);
      const auto& halfuRarr = halfuR.array(mfi);
      const auto& halfuLarr = halfuL.array(mfi);


      const auto& fluxArr = fluxes[d].array(mfi);
      const Real v = vel[d];

      ParallelFor(bx, [=] AMREX_GPU_DEVICE ( int i, int j, int k) noexcept
      		      {
      			// Conservative flux for the first-order
      			// backward difference method
      			// Try implement MHD

          // Slope limiting
          // Allocate full-state left/right arrays
          Real uLbar[NUM_STATE];
          Real uRbar[NUM_STATE];
          for(int n = 0; n < NUM_STATE; ++n)
          {
            Real duL = arr(i , j, k, n) - arr(i - iOffset, j - jOffset, k - kOffset, n);
            Real duR = arr(i + iOffset, j + jOffset, k + kOffset, n) - arr(i, j, k, n);

            Real r = (amrex::Math::abs(duR) < 1e-12) ? 0.0 : duL / duR;
            Real xi = minmod(r);

            Real deltai = 0.5 * duL + 0.5 * duR;

            uLbar[n] = arr(i, j, k, n) - 0.5 * xi * deltai;
            uRbar[n] = arr(i, j, k, n) + 0.5 * xi * deltai;

            //Compute left and right flux function arrays
            // left_flux_function = fluxfunction(uLbar[i][j], gamma, ch);
            // right_flux_function = fluxfunction(uRbar[i][j], gamma, ch);
            // Require flux function to know direction within the loop of amrex space dim i.e. if x then flux function needs to be in the x direction

            // Real half_uLbar = uLbar - (0.5)*(dt/dx)*(right_flux_function - left_flux_function);
          }
          Real fluxL[NUM_STATE];
          Real fluxR[NUM_STATE];

          fluxfunctionMHD(uLbar, fluxL, d, gam, ch);
          fluxfunctionMHD(uRbar, fluxR, d, gam, ch);

          // Real halfuL[NUM_STATE];
          // Real halfuR[NUM_STATE];

          for (int n = 0; n < NUM_STATE; ++n) {
            halfuLarr(i,j,k,n) = uLbar[n] - 0.5 * dt_by_dx * (fluxR[n] - fluxL[n]);
            halfuRarr(i,j,k,n) = uRbar[n] - 0.5 * dt_by_dx * (fluxR[n] - fluxL[n]);
        }
      });
    }
    halfuL.FillBoundary(geom.periodicity());
    halfuR.FillBoundary(geom.periodicity());
    
    //for (MFIter mfi(Sborder); mfi.isValid(); ++mfi)
    // This loop is going over all the cells, when it might need to go over the faces instead.
    for (MFIter mfi(fluxes[d], true); mfi.isValid(); ++mfi)
    {
      const auto& halfuRarr = halfuR.array(mfi);
      const auto& halfuLarr = halfuL.array(mfi);
      const Box& bx = mfi.tilebox();
      //std::cout << bx << std::endl;
      const auto& fluxArr = fluxes[d].array(mfi);


        ParallelFor(bx, [=] AMREX_GPU_DEVICE (int i, int j, int k) noexcept{
          Real fluxOut[NUM_STATE];
          Real halfuL_local[NUM_STATE];
          Real halfuR_local[NUM_STATE];
          
          for (int n = 0; n < NUM_STATE; n++)
          {
            halfuL_local[n] = halfuLarr(i,j,k,n);
            halfuR_local[n] = halfuRarr(i -iOffset, j - jOffset, k -kOffset,n);
          }
          HLLCFlux(halfuR_local, halfuL_local, fluxOut, d, gam, ch);

        for (int n = 0; n < NUM_STATE; ++n) {
          fluxArr(i,j,k,n) = fluxOut[n];
      }
          

          // half_uLbar[i][j][k] = uLbar[i][j][k] - (0.5)*(dt/dx)*(right_flux_function[i][j][k] - left_flux_function[i][j][k]);
          // half_uRbar[i][j][k] = uRbar[i][j][k] - (0.5)*(dt/dx)*(right_flux_function[i][j][k] - left_flux_function[i][j][k]);
      });
           
    }

    // Now we have fluxes, we can update data
    for (MFIter mfi(Sborder); mfi.isValid(); ++mfi)
    {
      const Box& bx = mfi.tilebox();

      // Indexable arrays for the data, and the directional flux
      // Based on the vertex-centred definition of the flux array, the
      // data array runs from e.g. [0,N] and the flux array from [0,N+1]
      const auto& arr = Sborder.array(mfi);
      const auto& fluxArr = fluxes[d].array(mfi);
      
      // exit(0);
      ParallelFor(bx, [=] AMREX_GPU_DEVICE ( int i, int j, int k) noexcept
      		      {
			// Conservative update formula
      for(int n = 0; n < NUM_STATE; n++)
      {
        arr(i,j,k, n) = arr(i,j,k, n)
			  - (dt_by_dx) * (fluxArr(i+iOffset, j+jOffset, k+kOffset, n)
					    - fluxArr(i,j,k, n));

              // if (j == 64 && i > 120 && n == 0)
              // {
              //   std::cout << "i = " << i << std::endl;
              //   std::cout << fluxArr(i,j,k,0) << " " << fluxArr(i+iOffset, j+jOffset, k+kOffset, 0) << " " << arr(i,j,k, 0) << std::endl;

              // }
      }
      arr(i,j,k, 8) = arr(i,j,k, 8) * exp((-ch*ch*dt)/(cp*cp));
		      });

    }

    // We need to compute boundary conditions again after each update
    Sborder.FillBoundary(geom.periodicity());  

    // The fluxes now need scaling for the reflux command.
    // This scaling is by the size of the boundary through which the flux passes, e.g. the x-flux needs scaling by the dy, dz and dt
    if(do_reflux)
    {
      Real scaleFactor = dt;
      for(int scaledir = 0; scaledir < amrex::SpaceDim; ++scaledir)
      {
	// Fluxes don't need scaling by dx[d]
	if(scaledir == d)
	{
	  continue;
	}
	scaleFactor *= dxVect[scaledir];
      }
      // The mult function automatically multiplies entries in a multifab by a scalar
      // scaleFactor: The scalar to multiply by
      // 0: The first data index in the multifab to multiply
      // NUM_STATE:  The total number of data indices that will be multiplied
      fluxes[d].mult(scaleFactor, 0, NUM_STATE);
    }
    
  }
  // DEBUGGING BLOCK: Check rho values in Sborder before the final copy
{
  amrex::Print() << "\n--- DEBUG: Checking Sborder right before MultiFab::Copy ---\n";
  const int rho_comp = 0; // Assuming rho is component 0

  // Create a host-side copy of Sborder to safely read its values on the CPU
  MultiFab Sborder_host(Sborder.boxArray(), Sborder.DistributionMap(),
                        Sborder.nComp(), Sborder.nGrowVect());
  MultiFab::Copy(Sborder_host, Sborder, 0, 0, Sborder.nComp(), Sborder.nGrowVect());
  
  for (MFIter mfi(Sborder_host); mfi.isValid(); ++mfi)
  {
      // We are interested in the valid region that will be copied.
      const Box& bx = mfi.tilebox();
      const auto& fab = Sborder_host.array(mfi);

      // Use amrex::Loop on the CPU to print values.
      // Capture mfi by reference [&] to avoid copy errors.
      amrex::Loop(bx, [&] (int i, int j, int k)
      {
          // Print for a specific cell to avoid too much output.
          // Let's check the corner cell where you saw "-nan".
          if (i == 0 && j == 0 && k == 0)
          {
              Real rho_val = fab(i, j, k, rho_comp);
              amrex::Print() << "Grid " << mfi.index() << ", Cell (" << i << "," << j << "," << k
                             << ") rho AFTER update = " << rho_val << "\n";
          }
      });
  }
  amrex::Print() << "--- DEBUG: End of Sborder check ---\n\n";
}

  // The updated data is now copied to the S_new multifab.  This means
  // it is now accessible through the get_new_data command, and AMReX
  // can automatically interpolate or extrapolate between layers etc.
  // S_new: Destination
  // Sborder: Source
  // Third entry: Starting variable in the source array to be copied (the zeroth variable in this case)
  // Fourth entry: Starting variable in the destination array to receive the copy (again zeroth here)
  // NUM_STATE: Total number of variables being copied
  // Sixth entry: Number of ghost cells to be included in the copy (zero in this case, since only real
  //              data is needed for S_new)
  MultiFab::Copy(S_new, Sborder, 0, 0, NUM_STATE, 0);

  // Refluxing at patch boundaries.  Amrex automatically does this
  // where needed, but you need to state a few things to make sure it
  // happens correctly:
  // FineAdd: If we are not on the coarsest level, the fluxes at this level will form part of the correction
  //          to a coarse level
  // CrseInit:  If we are not the finest level, the fluxes at patch boundaries need correcting.  Since we
  //            know that the coarse level happens first, we initialise the boundary fluxes through this
  //            function, and subsequently FineAdd will modify things ready for the correction
  // Both functions have the same arguments:
  // First: Name of the flux MultiFab (this is done dimension-by-dimension
  // Second: Direction, to ensure the correct vertices are being corrected
  // Third: Source component - the first entry of the flux MultiFab that is to be copied (it is possible that
  //        some variables will not need refluxing, or will be computed elsewhere (not in this example though)
  // Fourth: Destinatinon component - the first entry of the flux register that this call to FineAdd sends to
  // Fifth: NUM_STATE - number of states being added to the flux register
  // Sixth: Multiplier - in general, the least accurate (coarsest) flux is subtracted (-1) and the most
  //        accurate (finest) flux is added (+1)


  if (do_reflux) {
    if (current) {
      for (int i = 0; i < AMREX_SPACEDIM ; i++) {
	current->FineAdd(fluxes[i],i,0,0,NUM_STATE,1.);
      }
    }
    if (fine) {
      for (int i = 0; i < AMREX_SPACEDIM ; i++) {
	fine->CrseInit(fluxes[i],i,0,0,NUM_STATE,-1.);
      }
    }
  }

  return dt;
}

/**
 * Estimate time step.
 */
// This function is called by all of the other time step functions in AMReX, and is the only one that should
// need modifying
//
Real
AmrLevelAdv::estTimeStep (Real)
{
  const MultiFab& S_new = get_new_data(Phi_Type);
    GpuArray<Real, BL_SPACEDIM> dx = geom.CellSizeArray();

    // Call your wave speed function
    //const Real gam = 1.4;
    Real ch = computeMaxWaveSpeed(S_new, gam);

    // Compute time step using CFL condition
    Real dt_est = cfl * (*std::min_element(dx.begin(), dx.end())) / ch;

    // Reduce across all processors to get global minimum dt
    ParallelDescriptor::ReduceRealMin(dt_est);

    if (verbose) {
        amrex::Print() << "AmrLevelAdv::estTimeStep at level " << level
                       << ": max_wave_speed = " << ch
                       << ", dt_est = " << dt_est << std::endl;
    }

    return dt_est;
}

/**
 * Compute initial time step.
 */
Real
AmrLevelAdv::initialTimeStep ()
{
  return estTimeStep(0.0);
}

/**
 * Compute initial `dt'.
 */
void
AmrLevelAdv::computeInitialDt (int                   finest_level,
                               int                   /*sub_cycle*/,
                               Vector<int>&           n_cycle,
                               const Vector<IntVect>& /*ref_ratio*/,
                               Vector<Real>&          dt_level,
                               Real                  stop_time)
{
  //
  // Grids have been constructed, compute dt for all levels.
  //
  // AMReX's AMR Level mode assumes that the time step only needs
  // calculating on the coarsest level - all subsequent time steps are
  // reduced by the refinement factor
  if (level > 0) {
    return;
  }

  Real dt_0 = 1.0e+100;
  int n_factor = 1;
  for (int i = 0; i <= finest_level; i++)
  {
    dt_level[i] = getLevel(i).initialTimeStep();
    n_factor   *= n_cycle[i];
    dt_0 = std::min(dt_0,n_factor*dt_level[i]);
  }
  
  //
  // Limit dt's by the value of stop_time.
  //
  const Real eps = 0.001*dt_0;
  Real cur_time  = state[Phi_Type].curTime();
  if (stop_time >= 0.0) {
    if ((cur_time + dt_0) > (stop_time - eps)) {
      dt_0 = stop_time - cur_time;
    }
  }
  
  n_factor = 1;
  for (int i = 0; i <= finest_level; i++)
  {
    n_factor *= n_cycle[i];
    dt_level[i] = dt_0/n_factor;
  }
}

/**
 * Compute new `dt'.
 */
void
AmrLevelAdv::computeNewDt (int                   finest_level,
                           int                   /*sub_cycle*/,
                           Vector<int>&           n_cycle,
                           const Vector<IntVect>& /*ref_ratio*/,
                           Vector<Real>&          dt_min,
                           Vector<Real>&          dt_level,
                           Real                  stop_time,
                           int                   post_regrid_flag)
{
  //
  // We are at the end of a coarse grid timecycle.
  // Compute the timesteps for the next iteration.
  //
  if (level > 0) {
    return;
  }

  // Although we only compute the time step on the finest level, we
  // need to take information from all levels into account.  The
  // sharpest features may be smeared out on coarse levels, so not
  // using finer levels could cause instability
  for (int i = 0; i <= finest_level; i++)
  {
    AmrLevelAdv& adv_level = getLevel(i);
    dt_min[i] = adv_level.estTimeStep(dt_level[i]);
  }

  // A couple of things are implemented to ensure that time step's
  // don't suddenly grow by a lot, as this could lead to errors - for
  // sensible mesh refinement choices, these shouldn't really change
  // anything
  if (post_regrid_flag == 1)
  {
    //
    // Limit dt's by pre-regrid dt
    //
    for (int i = 0; i <= finest_level; i++)
    {
      dt_min[i] = std::min(dt_min[i],dt_level[i]);
    }
  }
  else
  {
    //
    // Limit dt's by change_max * old dt
    //
    static Real change_max = 1.1;
    for (int i = 0; i <= finest_level; i++)
    {
      dt_min[i] = std::min(dt_min[i],change_max*dt_level[i]);
    }
  }

  //
  // Find the minimum over all levels
  //
  Real dt_0 = 1.0e+100;
  int n_factor = 1;
  for (int i = 0; i <= finest_level; i++)
  {
    n_factor *= n_cycle[i];
    dt_0 = std::min(dt_0,n_factor*dt_min[i]);
  }

  //
  // Limit dt's by the value of stop_time.
  //
  const Real eps = 0.001*dt_0;
  Real cur_time  = state[Phi_Type].curTime();
  if (stop_time >= 0.0) {
    if ((cur_time + dt_0) > (stop_time - eps)) {
      dt_0 = stop_time - cur_time;
    }
  }

  n_factor = 1;
  for (int i = 0; i <= finest_level; i++)
  {
    n_factor *= n_cycle[i];
    dt_level[i] = dt_0/n_factor;
  }
}

/**
 * Do work after timestep().
 */
// If something has to wait until all processors have done their advance function, the post_timestep function
// is the place to put it.  Refluxing and averaging down are two standard examples for AMR
//
void
AmrLevelAdv::post_timestep (int iteration)
{
  //
  // Integration cycle on fine level grids is complete
  // do post_timestep stuff here.
  //
  int finest_level = parent->finestLevel();

  if (do_reflux && level < finest_level) {
    reflux();
  }

  if (level < finest_level) {
    avgDown();
  }

  if (level < finest_level) {
    // fillpatcher on level+1 needs to be reset because data on this
    // level have changed.
    getLevel(level+1).resetFillPatcher();
  }

}

/**
 * Do work after regrid().
 */
// Nothing normally needs doing here, but if something was calculated on a per-patch basis, new patches might
// this to be calcuated immediately
//
void
AmrLevelAdv::post_regrid (int lbase, int /*new_finest*/) {
  // Prevent warnings about unused variables
  amrex::ignore_unused(lbase);
}

/**
 * Do work after a restart().
 */
void
AmrLevelAdv::post_restart()
{
}

/**
 * Do work after init().
 */
// Once new patches have been initialised, work may need to be done to ensure consistency, for example,
// averaging down - though for linear interpolation, this probably won't change anything
void
AmrLevelAdv::post_init (Real /*stop_time*/)
{
  if (level > 0) {
    return;
  }
  //
  // Average data down from finer levels
  // so that conserved data is consistent between levels.
  //
  int finest_level = parent->finestLevel();
  for (int k = finest_level-1; k>= 0; k--) {
    getLevel(k).avgDown();
  }
}

/**
 * Error estimation for regridding.
 */
//  Determine which parts of the domain need refinement
void
AmrLevelAdv::errorEst (TagBoxArray& tags,
                       int          /*clearval*/,
                       int          /*tagval*/,
                       Real         /*time*/,
                       int          /*n_error_buf*/,
                       int          /*ngrow*/)
{
  MultiFab& S_new = get_new_data(Phi_Type);
  
  // Properly fill patches and ghost cells for phi gradient check
  // (i.e. make sure there is at least one ghost cell defined)
  const int oneGhost = 1;
  MultiFab phitmp;
  if (level < max_phigrad_lev)
  {
    // Only do this if we actually compute errors based on gradient at this level
    const Real cur_time = state[Phi_Type].curTime();
    phitmp.define(S_new.boxArray(), S_new.DistributionMap(), NUM_STATE, 1);
    FillPatch(*this, phitmp, oneGhost, cur_time, Phi_Type, 0, NUM_STATE);
  }
  MultiFab const& phi = (level < max_phigrad_lev) ? phitmp : S_new;
  
  const char   tagval = TagBox::SET;
  // const char clearval = TagBox::CLEAR;
  
#ifdef AMREX_USE_OMP
#pragma omp parallel if (Gpu::notInLaunchRegion())
#endif
    {
      for (MFIter mfi(phi,TilingIfNotGPU()); mfi.isValid(); ++mfi)
      {
	const Box& tilebx  = mfi.tilebox();
	const auto phiarr  = phi.array(mfi);
	auto       tagarr  = tags.array(mfi);
	
	// Tag cells with high phi.
	if (level < max_phierr_lev) {
	  const Real phierr_lev  = phierr[level];
	  amrex::ParallelFor(tilebx,
			     [=] AMREX_GPU_DEVICE (int i, int j, int k) noexcept
			     {
			       state_error(i, j, k, tagarr, phiarr, phierr_lev, tagval);
			     });
	}
	
	// Tag cells with high phi gradient.
	if (level < max_phigrad_lev) {
	  const Real phigrad_lev = phigrad[level];
	  amrex::ParallelFor(tilebx,
			     [=] AMREX_GPU_DEVICE (int i, int j, int k) noexcept
			     {
			       grad_error(i, j, k, tagarr, phiarr, phigrad_lev, tagval);
			     });
	}
      }
    }
}

/**
 * Read parameters from input file.
 */
void
AmrLevelAdv::read_params ()
{
  // Make sure that this is only done once
  static bool done = false;

  if (done) { return; }

  done = true;

  // A ParmParse object allows settings, with the correct prefix, to be read in from the settings file
  // The prefix can help identify what a settings parameter is used for
  // AMReX has some default ParmParse names, amr and geometry are two commonly needed ones
  ParmParse pp("adv");

  // A ParmParse object allows settings, with the correct prefix, to be read in from the settings file
  // The prefix can help identify what a settings parameter is used for
  // AMReX has some default ParmParse names, amr and geometry are two commonly needed ones
  pp.query("v",verbose);
  pp.query("cfl",cfl);
  pp.query("do_reflux",do_reflux);

  // Vector variables can be read in; these require e.g.\ pp.queryarr
  // and pp.getarr, so that the ParmParse object knows to look for
  // more than one variable

  // Geometries can be Cartesian, cylindrical or spherical - some
  // functions (e.g. divergence in linear solvers) are coded with this
  // geometric dependency
  Geometry const* gg = AMReX::top()->getDefaultGeometry();

  // This tutorial code only supports Cartesian coordinates.
  if (! gg->IsCartesian()) {
    amrex::Abort("Please set geom.coord_sys = 0");
  }

  // This tutorial code only supports periodic boundaries.
  // The periodicity is read from the settings file in AMReX source code, but can be accessed here

  // if (! gg->isAllPeriodic()) {
  //   amrex::Abort("Please set geom.is_periodic = 1 1 1");
  // }

  // Read tagging parameters from tagging block in the input file.
  // See Src_nd/Tagging_params.cpp for the function implementation.
  get_tagging_params();
}


//
// AMReX has an inbuilt reflux command, but we still have the freedom
// to decide what goes into it (for example, which variables are
// actually refluxed).  This also gives a little flexibility as to
// where flux registers are stored.  In this example, they are stored
// on levels [1,fine] but not level 0.  
//
void
AmrLevelAdv::reflux ()
{
  BL_ASSERT(level<parent->finestLevel());

  const auto strt = amrex::second();

  // Call the reflux command with the appropriate data.  Because there
  // are no flux registers on the coarse level, they start from the
  // first level.  But the coarse level to the (n-1)^th are the ones
  // that need refluxing, hence the `level+1'.  
  getFluxReg(level+1).Reflux(get_new_data(Phi_Type),1.0,0,0,NUM_STATE,geom);

  if (verbose)
  {
    const int IOProc = ParallelDescriptor::IOProcessorNumber();
    auto      end    = amrex::second() - strt;

    ParallelDescriptor::ReduceRealMax(end,IOProc);

    amrex::Print() << "AmrLevelAdv::reflux() at level " << level
		   << " : time = " << end << std::endl;
  }
}

//
// Generic function for averaging down - in this case it just makes sure it doesn't happen on the finest level
//
void
AmrLevelAdv::avgDown ()
{
  if (level == parent->finestLevel()) { return; }
  // Can select which variables averaging down will happen on - only one to choose from in this case!
  avgDown(Phi_Type);
}

//
// Setting up the call to the AMReX-implemented average down function
//
void
AmrLevelAdv::avgDown (int state_indx)
{
  // For safety, again make sure this only happens if a finer level exists
  if (level == parent->finestLevel()) { return; }
  
  // You can access data at other refinement levels, use this to
  // specify your current data, and the finer data that is to be
  // averaged down
  AmrLevelAdv& fine_lev = getLevel(level+1);
  MultiFab&  S_fine   = fine_lev.get_new_data(state_indx);
  MultiFab&  S_crse   = get_new_data(state_indx);
  
  // Call the AMReX average down function:
  // S_fine: Multifab with the fine data to be averaged down
  // S_crse: Multifab with the coarse data to receive the fine data where necessary
  // fine_lev.geom:  Geometric information (cell size etc.) for the fine level
  // geom: Geometric information for the coarse level (i.e. this level)
  // 0: First variable to be averaged (as not all variables need averaging down
  // S_fine.nComp(): Number of variables to average - this can be computed automatically from a multifab
  // refRatio: The refinement ratio between this level and the finer level
  amrex::average_down(S_fine,S_crse,
		      fine_lev.geom,geom,
		      0,S_fine.nComp(),parent->refRatio(level));
}

AMREX_GPU_HOST_DEVICE
void AmrLevelAdv::fluxfunctionMHD(const Real* u, Real* flux, int dir, Real gamma, Real ch)
{
    Real rho = u[0];
    Real vx = u[1] / rho;
    Real vy = u[2] / rho;
    Real vz = u[3] / rho;
    Real Bx = u[4];
    Real By = u[5];
    Real Bz = u[6];
    Real E  = u[7];

    Real v2 = vx*vx + vy*vy + vz*vz;
    Real B2 = Bx*Bx + By*By + Bz*Bz;
    Real p  = (gamma - 1.0) * (E - 0.5 * rho * v2 - 0.5 * B2);

    // Dot product vÂ·B
    Real vdotB = vx*Bx + vy*By + vz*Bz;

    // Zero initialize flux
    for (int n = 0; n < NUM_STATE; ++n) flux[n] = 0.0;

    if (dir == 0) { // X-direction
        flux[0] = u[1];
        flux[1] = rho * vx*vx + p + 0.5 * B2 - Bx*Bx;
        flux[2] = rho * vx*vy - Bx*By;
        flux[3] = rho * vx*vz - Bx*Bz;
        flux[4] = u[8];
        flux[5] = vx*By - vy*Bx;
        flux[6] = vx*Bz - vz*Bx;
        flux[7] = (E + p + 0.5 * B2)*vx - vdotB*Bx;
        flux[8] = (ch*ch) * Bx;

    } else if (dir == 1) { // Y-direction
        flux[0] = u[2];
        flux[1] = rho * vy*vx - By*Bx;
        flux[2] = rho * vy*vy + p + 0.5 * B2 - By*By;
        flux[3] = rho * vy*vz - By*Bz;
        flux[4] = vy*Bx - vx*By;
        flux[5] = u[8];
        flux[6] = vy*Bz - vz*By;
        flux[7] = (E + p + 0.5 * B2)*vy - vdotB*By;
        flux[8] = (ch*ch)*By;

    } else if (dir == 2) { // Z-direction
        flux[0] = u[3];
        flux[1] = rho * vz*vx - Bz*Bx;
        flux[2] = rho * vz*vy - Bz*By;
        flux[3] = rho * vz*vz + p + 0.5 * B2 - Bz*Bz;
        flux[4] = vz*Bx - vx*Bz;
        flux[5] = vz*By - vy*Bz;
        flux[6] = u[8];
        flux[7] = (E + p + 0.5 * B2)*vz - vdotB*Bz;
        flux[8] = (ch*ch)*Bz;
    }

    // If divergence cleaning is enabled (optional), add PSICONS update here if used
};

AMREX_GPU_HOST_DEVICE 
void AmrLevelAdv::ConservativeToPrimitive(const Real* u, const Real gamma, Real* prim)
{
    // Convert conservative variables to primitive variables
    prim[0] = u[0];  // Density
    prim[1] = u[1] / u[0];  // Velocity in x
    prim[2] = u[2] / u[0];  // Velocity in y
    prim[3] = u[3] / u[0];  // Velocity in z
    prim[4] = u[4];
    prim[5] = u[5];
    prim[6] = u[6];
    prim[7] = (gamma - 1)*(u[7] - ((u[1])*(u[1]) + (u[2]*u[2]) + (u[3]*u[3]))/(2*u[0]) - (0.5) * (u[4]*u[4] + u[5]*u[5] + u[6]*u[6]));
    // Include pressure computation or any additional primitive variables
}

Real AmrLevelAdv::computeMaxWaveSpeed(const MultiFab& S, Real gamma) const
{
    using namespace amrex;

    // Set up a parallel reduction for max
    ReduceOps<ReduceOpMax> reduce_op;
    ReduceData<Real> reduce_data(reduce_op);

    for (MFIter mfi(S, TilingIfNotGPU()); mfi.isValid(); ++mfi)
    {
        const Box& bx = mfi.tilebox();
        const auto& arr = S.const_array(mfi);

        reduce_op.eval(bx, reduce_data,
            [=] AMREX_GPU_DEVICE (int i, int j, int k) -> GpuTuple<Real>
        {
            const Real rho = arr(i,j,k,0);
            const Real vx  = arr(i,j,k,1) / rho;
            const Real vy  = arr(i,j,k,2) / rho;
            const Real vz  = arr(i,j,k,3) / rho;

            const Real Bx = arr(i,j,k,4);
            const Real By = arr(i,j,k,5);
            const Real Bz = arr(i,j,k,6);
            const Real energy = arr(i,j,k,7);

            const Real kinetic = 0.5 * rho * (vx*vx + vy*vy + vz*vz);
            const Real magnetic = 0.5 * (Bx*Bx + By*By + Bz*Bz);
            const Real pressure = (gamma - 1.0) * (energy - kinetic - magnetic);
            //std::cout << "i = " <<  i << ", j = " << j << "k = " << k << std::endl;
            // Avoid unphysical pressure (optional guard)
            if (pressure <= 0.0 || rho <= 0.0) return {0.0};

            const Real cs = std::sqrt(gamma * pressure / rho);
            const Real Bmag2 = Bx*Bx + By*By + Bz*Bz;

            const Real term = cs*cs + Bmag2/rho;

            const Real cf_x = std::sqrt(0.5 * (term + std::sqrt(term*term - 4.0 * cs*cs * Bx*Bx / rho)));
            const Real cf_y = std::sqrt(0.5 * (term + std::sqrt(term*term - 4.0 * cs*cs * By*By / rho)));
            const Real cf_z = std::sqrt(0.5 * (term + std::sqrt(term*term - 4.0 * cs*cs * Bz*Bz / rho)));

            const Real speed_x = std::abs(vx) + cf_x;
            const Real speed_y = std::abs(vy) + cf_y;
            const Real speed_z = std::abs(vz) + cf_z;

            const Real local_max = amrex::max(speed_x, speed_y, speed_z);
            return {local_max};
        });
    }

    // Extract the final maximum value
    return amrex::get<0>(reduce_data.value());
}

AMREX_GPU_HOST_DEVICE void AmrLevelAdv::HLLCFlux(Real* uL, Real* uR, Real* fluxOut, int dir, Real Gamma, Real ch)
{
  // Define indices for clarity, assuming NUM_STATE = 9
  const int E_idx = 7;
  const int psi_idx = 8;

  // Make local copies to prevent modifying the original input arrays.
  // All calculations will be done on these 'cleaned' states.
  Real uL_clean[NUM_STATE];
  Real uR_clean[NUM_STATE];
  for (int i = 0; i < NUM_STATE; ++i) {
    uL_clean[i] = uL[i];
    uR_clean[i] = uR[i];
  }

  // --- Divergence Cleaning (Powell's method) ---
  // Apply if ch is positive, matching the logic from the reference code.
  if (ch > 0.0)
  {
    const int B_norm_idx = 4 + dir;
    Real Btilda   = 0.5 * (uL_clean[B_norm_idx] + uR_clean[B_norm_idx]) - 0.5 / ch * (uR_clean[psi_idx] - uL_clean[psi_idx]);
    Real Psitilda = 0.5 * (uL_clean[psi_idx]   + uR_clean[psi_idx])   - 0.5 * ch * (uR_clean[B_norm_idx] - uL_clean[B_norm_idx]);
    
    uL_clean[B_norm_idx] = Btilda;
    uR_clean[B_norm_idx] = Btilda;
    uL_clean[psi_idx]    = Psitilda;
    uR_clean[psi_idx]    = Psitilda;
  }

  // --- Calculations now proceed with the cleaned states ---

  // Convert the cleaned conservative states to primitive variables.
  Real primL[NUM_STATE];
  Real primR[NUM_STATE];
  AmrLevelAdv::ConservativeToPrimitive(uL_clean, Gamma, primL);
  AmrLevelAdv::ConservativeToPrimitive(uR_clean, Gamma, primR);

  // Compute fluxes from the cleaned states.
  Real FL[NUM_STATE];
  Real FR[NUM_STATE];
  fluxfunctionMHD(uL_clean, FL, dir, Gamma, ch);
  fluxfunctionMHD(uR_clean, FR, dir, Gamma, ch);

  // Extract normal velocities from primitive states.
  Real vL = primL[1 + dir];
  Real vR = primR[1 + dir];

  // Extract densities
  Real rhoL = primL[0];
  Real rhoR = primR[0];

  // Extract pressures
  Real pL = primL[E_idx];
  Real pR = primR[E_idx];

  // Extract magnetic field components and calculate fast magnetosonic speeds.
  Real cfL = ComputeFastMagnetosonicSpeed(primL, dir, Gamma);
  Real cfR = ComputeFastMagnetosonicSpeed(primR, dir, Gamma);

  // Left and right wave speeds (Davis, 1988).
  Real SL = amrex::min(vL, vR) - amrex::max(cfL, cfR);
  Real SR = amrex::max(vL, vR) + amrex::max(cfL, cfR);

  // Extract normal momentum from cleaned conservative states.
  Real momL_norm = uL_clean[1 + dir];
  Real momR_norm = uR_clean[1 + dir];

  // Total pressures (thermal + magnetic)
  Real pT_l = pL + 0.5 * (primL[4]*primL[4] + primL[5]*primL[5] + primL[6]*primL[6]);
  Real pT_r = pR + 0.5 * (primR[4]*primR[4] + primR[5]*primR[5] + primR[6]*primR[6]);

  // Normal component of magnetic field from primitive states
  Real BnL = primL[4 + dir];
  Real BnR = primR[4 + dir];

  // Compute S_star (star region wave speed)
  Real numerator = momR_norm * (SR - vR) - momL_norm * (SL - vL) + pT_l - pT_r - (BnL*BnL - BnR*BnR);
  Real denominator = rhoR * (SR - vR) - rhoL * (SL - vL);
  Real S_star = numerator / denominator;

  // Compute HLL average state.
  Real uHLL[NUM_STATE];
  for (int n = 0; n < NUM_STATE; ++n)
  {
      uHLL[n] = (SR * uR_clean[n] - SL * uL_clean[n] + FL[n] - FR[n]) / (SR - SL);
  }

  // --- Compute Left Star State (uHLLC_L) ---
  Real uHLLC_L[NUM_STATE];

  // B-field and Psi are taken from the HLL state.
  uHLLC_L[4] = uHLL[4];
  uHLLC_L[5] = uHLL[5];
  uHLLC_L[6] = uHLL[6];
  uHLLC_L[psi_idx] = uHLL[psi_idx];
  
  // Total pressure in the star region (from the left state's perspective).
  Real pT_HLLC_L = rhoL * (SL - vL) * (S_star - vL) + pT_l - BnL*BnL + (uHLLC_L[4 + dir] * uHLLC_L[4 + dir]);
  
  Real coeffL = rhoL * (SL - vL) / (SL - S_star);
  uHLLC_L[0] = coeffL; // Density

  // Momentum components in the star region.
  for (int i = 0; i < 3; ++i) 
  {
    if (i == dir) // Normal momentum
    {
      uHLLC_L[1 + i] = coeffL * S_star;
    }
    else // Tangential momentum
    {
      uHLLC_L[1 + i] = uL_clean[1 + i] * (SL - vL) / (SL - S_star) 
                     - (uHLL[4 + dir] * uHLL[4 + i] - BnL * uL_clean[4 + i]) / (SL - S_star);
    }
  }

  // Energy in the star region
  Real B_dot_v_star_L = (uHLLC_L[4] * (uHLLC_L[1]/uHLLC_L[0]) + uHLLC_L[5] * (uHLLC_L[2]/uHLLC_L[0]) + uHLLC_L[6] * (uHLLC_L[3]/uHLLC_L[0]));
  Real B_dot_v_L      = (primL[4] * primL[1] + primL[5] * primL[2] + primL[6] * primL[3]);
  uHLLC_L[E_idx] = uL_clean[E_idx] * (SL - vL) / (SL - S_star) 
                 + (pT_HLLC_L * S_star - pT_l * vL - (uHLLC_L[4 + dir] * B_dot_v_star_L - BnL * B_dot_v_L)) / (SL - S_star);


  // --- Compute Right Star State (uHLLC_R) ---
  Real uHLLC_R[NUM_STATE];

  // B-field and Psi are taken from the HLL state.
  uHLLC_R[4] = uHLL[4];
  uHLLC_R[5] = uHLL[5];
  uHLLC_R[6] = uHLL[6];
  uHLLC_R[psi_idx] = uHLL[psi_idx];

  // Total pressure in the star region (from the right state's perspective).
  Real pT_HLLC_R = rhoR * (SR - vR) * (S_star - vR) + pT_r - BnR*BnR + (uHLLC_R[4 + dir] * uHLLC_R[4 + dir]);

  Real coeffR = rhoR * (SR - vR) / (SR - S_star);
  uHLLC_R[0] = coeffR; // Density

  // Momentum components
  for (int i = 0; i < 3; ++i)
  {
    if (i == dir) // Normal momentum
    {
      uHLLC_R[1 + i] = coeffR * S_star;
    }
    else // Tangential momentum
    {
      uHLLC_R[1 + i] = uR_clean[1 + i] * (SR - vR) / (SR - S_star) 
                     - (uHLL[4 + dir] * uHLL[4 + i] - BnR * uR_clean[4 + i]) / (SR - S_star);
    }
  }
  
  // Energy in the star region
  Real B_dot_v_star_R = (uHLLC_R[4] * (uHLLC_R[1]/uHLLC_R[0]) + uHLLC_R[5] * (uHLLC_R[2]/uHLLC_R[0]) + uHLLC_R[6] * (uHLLC_R[3]/uHLLC_R[0]));
  Real B_dot_v_R      = (primR[4] * primR[1] + primR[5] * primR[2] + primR[6] * primR[3]);
  uHLLC_R[E_idx] = uR_clean[E_idx] * (SR - vR) / (SR - S_star)
                 + (pT_HLLC_R * S_star - pT_r * vR - (uHLLC_R[4 + dir] * B_dot_v_star_R - BnR * B_dot_v_R)) / (SR - S_star);


  // --- Final HLLC Flux Selection ---
  // The flux is selected based on the wave speeds. Note that we use the cleaned
  // states and their corresponding fluxes (FL, FR) for the final result.
  if (SL >= 0)
  {
    for (int i = 0; i < NUM_STATE; i++) fluxOut[i] = FL[i];
  }
  else if (S_star >= 0 && SL < 0) // Note: SL<=0 in original, changed to < to avoid division by zero if SL=S_star
  {
    for (int i = 0; i < NUM_STATE; i++)
    {
      fluxOut[i] = FL[i] + SL * (uHLLC_L[i] - uL_clean[i]);
    }
  }
  else if (S_star < 0 && SR >= 0) // Note: S_star<=0 in original
  {
    for (int i = 0; i < NUM_STATE; i++)
    {
      fluxOut[i] = FR[i] + SR * (uHLLC_R[i] - uR_clean[i]);
    }
  }
  else // SR < 0
  {
    for (int i = 0; i < NUM_STATE; i++) fluxOut[i] = FR[i];
  }
}

AMREX_GPU_HOST_DEVICE
Real AmrLevelAdv::ComputeFastMagnetosonicSpeed(const Real* prim, int dir, Real gamma)
{
    Real rho = prim[0];
    Real p   = prim[7];

    // Magnetic field components
    Real Bx = prim[4];
    Real By = prim[5];
    Real Bz = prim[6];

    // Directional magnetic field component
    Real Bn = prim[4 + dir]; // Bx, By, or Bz depending on dir

    // Alfven speeds squared
    Real vA_sq = (Bx*Bx + By*By + Bz*Bz) / rho;
    Real vA_n_sq = (Bn*Bn) / rho;

    // Sound speed squared
    Real a_sq = gamma * p / rho;

    // Fast magnetosonic speed formula
    Real term = sqrt((a_sq + vA_sq) * (a_sq + vA_sq) - 4.0 * a_sq * vA_n_sq);
    Real cf = sqrt(0.5 * (a_sq + vA_sq + term));

    return cf;
}
